2023-12-12 20:16:42,917 - INFO - Session path: save/maestro_2008_2011.sess
2023-12-12 20:16:42,917 - INFO - Dataset path: dataset/processed/maestro_2008_2011
2023-12-12 20:16:42,917 - INFO - Saving interval: 60.0
2023-12-12 20:16:42,917 - INFO - Hyperparameters: init_dim=32,event_dim=240,control_dim=24,hidden_dim=512,gru_layers=3,gru_dropout=0.3
2023-12-12 20:16:42,917 - INFO - Learning rate: 0.001
2023-12-12 20:16:42,917 - INFO - Batch size: 128
2023-12-12 20:16:42,917 - INFO - Window size: 200
2023-12-12 20:16:42,917 - INFO - Stride size: 10
2023-12-12 20:16:42,917 - INFO - Control ratio: 0.3
2023-12-12 20:16:42,917 - INFO - Teacher forcing ratio: 1.0
2023-12-12 20:16:42,917 - INFO - Random transposition: False
2023-12-12 20:16:42,917 - INFO - Reset optimizer: False
2023-12-12 20:16:42,917 - INFO - Enabling logging: False
2023-12-12 20:16:42,917 - INFO - Device: cpu
2023-12-12 20:16:43,463 - INFO - Loading session
2023-12-12 20:16:43,475 - INFO - Session is loaded from save/maestro_2008_2011.sess
2023-12-12 20:16:43,693 - INFO - PerformanceRNN(
  (inithid_fc): Linear(in_features=32, out_features=1536, bias=True)
  (inithid_fc_activation): Tanh()
  (event_embedding): Embedding(240, 240)
  (concat_input_fc): Linear(in_features=265, out_features=512, bias=True)
  (concat_input_fc_activation): LeakyReLU(negative_slope=0.1, inplace=True)
  (gru): GRU(512, 512, num_layers=3, dropout=0.3)
  (output_fc): Linear(in_features=1536, out_features=240, bias=True)
  (output_fc_activation): Softmax(dim=-1)
)
2023-12-12 20:16:43,694 - INFO - Loading dataset
2023-12-12 20:16:44,026 - INFO - Dataset(root="dataset/processed/maestro_2008_2011", samples=310, avglen=16159.593548387096)
2023-12-12 20:17:21,707 - INFO - iter 0, loss: 4.95695686340332
2023-12-12 20:17:58,994 - INFO - iter 1, loss: 4.958590507507324
2023-12-12 20:18:49,984 - INFO - iter 2, loss: 4.965318202972412
2023-12-12 20:19:40,654 - INFO - iter 3, loss: 4.937756061553955
2023-12-12 20:20:21,428 - INFO - iter 4, loss: 4.9557719230651855
2023-12-12 20:21:07,769 - INFO - iter 5, loss: 4.942343235015869
2023-12-12 20:22:00,035 - INFO - iter 6, loss: 4.952176094055176
2023-12-12 20:22:51,827 - INFO - iter 7, loss: 4.9366888999938965
2023-12-12 20:23:33,724 - INFO - iter 8, loss: 4.920251846313477
2023-12-12 20:24:28,319 - INFO - iter 9, loss: 4.928055286407471
2023-12-12 20:25:13,239 - INFO - iter 10, loss: 4.923009872436523
